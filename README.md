# Poisoning-Attack-Defense-in-Python
# Abstract
Our defense mechanism integrates the FRIENDS algorithm and gradient shaping techniques, as proposed in the FRIENDS paper [2] and the gradient shaping paper [1], respectively. In our experiments, we utilized a curated dataset, incorporating a diverse range of real estate listings, to evaluate the model's resilience against adversarial attacks. Specifically, we employed a poisoning attack by injecting instances of manipulated housing prices. A poisoning attack is considered successful if the MSE value of the regression exceeds the value 2.0. By tuning the noise and the magnitude of gradient shaping, we achieved a notable reduction in the attack success rate, reaching average MSEs as low as 0.566 and attack success rates of 6 % . 

# Introduction
In addition to addressing adversarial attacks in classification tasks, our work acknowledges the significance of gradient masking, as discussed in previous studies [2]. Recognizing the prevalence of adversarial attacks in linear regression models [3], we propose a comprehensive defense approach that combines the FRIENDS algorithm with gradient shaping. To enhance the structure, we introduce a new paragraph before "We seek to explore..." to elucidate our method. This section details our approach, involving both the FRIENDS algorithm and gradient shaping, applied to a dataset of real estate listings. We refer to existing literature [1] supporting FRIENDS as an efficient defense mechanism against poisoning attacks. Our implementation incorporates formalism to elucidate the application of gradient shaping and FRIENDS. We provide detailed explanations of the 'clip_threshold' parameter, its significance in gradient clipping, and the rationale for utilizing a Laplace function to generate noise. We've removed suggestions for further work related to data quality, complexity, and different models, focusing on the necessity of combining FRIENDS with gradient shaping. The linear regression model used is now explicitly stated, along with the formalism for theta_0 and theta_1. Our optimization process, including the learning rate and number of epochs, is clarified, considering the model's limited size and dataset. Detailed information, such as how the actual house price is determined, the dataset's size, the ratio of poisoned data, and the model's complexity, has been incorporated. Visualizations of the dataset are included for clarity.

# Literature Review
In a paper that proposed a defense strategy named “gradient shaping” which aims to constrain gradient magnitudes, Hong et al. employed differentially private stochastic gradient descent (DP-SGD) as a representative tool. The research presented in this paper focuses on two specific scenarios, highlighting the impact of poisoning attacks on model training and re-training. These scenarios represent common instances of indiscriminate and targeted poisoning attacks, respectively, each showing distinct aspects of the poisoning phenomenon. Although the paper's introduction of "gradient shaping" as a defense approach demonstrates innovation in countering poisoning attacks, we realize that it does not fully capture the complexity of real-world poisoning attacks, potentially limiting the generalizability of its findings. Further research could explore the integration of real-world factors, such as data quality and model complexity, to further evaluate defense mechanisms. We believe future research could also extend gradient shaping strategies to different machine learning models and tasks, enhancing our understanding of its applicability and limitations. A similar paper introduced a defense mechanism named FRIENDS that defends against various types of poisoning attacks on deep learning models. Liu et al. demonstrated that this defense mechanism has high effectiveness in breaking various invisible poisoning attacks with minimal impact on model performance. It utilizes two components to break poisons: accuracy-friendly perturbation and random varying noise. This combination has made this defense mechanism highly effective, leading us to implement it into our algorithm. We aim to demonstrate the effectiveness of FRIENDS as part of a comprehensive defense approach that specifically protects linear regression models from poisoning attacks. In another paper about security attacks in machine learning systems, Xiong et al. emphasized the need for trustworthy ML system development. This resonates with our objective to propose a defense mechanism that enhances the robustness of linear regression models and maintains their performance in the presence of adversarial attacks. The paper also goes into detail about the importance of efficient ML defensive technologies. We align with this concern and will focus on the implementation and evaluation of defense mechanisms, aiming to mitigate the impact of poisoning attacks on linear regression models.

# Methodology
_3.1 Basic Overview_

The code aims to evaluate the attack success rate over multiple trials (defined by the range 1,51 in the program). In each trial, a random data point with input features and corresponding output is chosen as a malicious data point. The model undergoes training with poisoned data and is fine-tuned with additional noise. The predicted output for the malicious input is then compared to the actual output, and an attack is considered successful if the difference exceeds a predefined threshold (set to 1 in this case). Then, the attack’s MSE value is calculated, an attack is considered successful if the MSE value is greater than 3.

Data is loaded from a CSV file (e.g., 'winequality-white.csv')[4]. The model is initialized with hyperparameters such as learning_rate_combined (set to 0.001) and epochs (set to 100,000), controlling the optimization process. The bias and weight of the linear regression model are controlled by theta_0 and theta_1, respectively. Perturbations and noise are generated using noise_std and clip_threshold. The noise_std variable controls the standard deviation of the noise added to the model’s gradients during the poisoning attack, while clip_threshold represents the threshold value used to clip the gradients during the attack.

The primary function, train_with_combined_optimizer(), implements the training process using a combined optimization algorithm. It accumulates the gradients for theta_0 and theta_1 over each data point in the training set and then updates the parameters using the gradients and the learning rate. Within each iteration, the function introduces random noise to the gradients before updating the parameters.
To test the model’s effectiveness against poisoning attacks, the perform_poisoning_attack() function is introduced. This function performs a poisoning attack by appending a malicious data point (input features and corresponding output) to the original data and then calls the training function to update the model with the poisoned data.


_3.2 Models and Optimization_

While the paper on gradient shaping provides valuable insights, it may not comprehensively address the intricacies of real-world poisoning attacks. To complement this, we integrate the FRIENDS method. The decision to include FRIENDS is justified by its ability to mitigate specific attack vectors that may not be fully covered by gradient shaping alone.

The linear regression model employed is based on Ordinary Least Squares (OLS). In this model, theta_0 represents the bias term, and theta_1 represents the weight of the input feature. The model predicts the output by combining the bias and the weighted input feature.

To optimize the combined optimizer, we carefully choose the learning rate (learning_rate_combined) and set the number of epochs to 100,000. These hyperparameters are selected to ensure a balance between convergence speed and avoiding overshooting. The optimization process iteratively updates the model parameters (theta_0 and theta_1) based on the gradients.

_3.3 Dataset Used_

The dataset we used to test the program and fine-tune values is from the UCI Machine Learning Archive’s Wine Quality Dataset [4], specifically the one for white wine, consisting of 4898 rows. The input values were from the fixed acidity column and the output values were from the volatile acidity column.


# Design Specification
We have introduced a systematic performance evaluation process, using MSE, to assess the impact of noise on overall model performance. By aligning the noise addition process with optimization principles from Geiping et al's paper, we aim to maximize perturbation of poisoned data points without compromising the model's effectiveness.
The gradient shaping implementation explicitly integrates the ratio of the magnitude of poisoned gradients to clean gradients and considers orientation differences using the cosine similarity score. This enhancement ensures a clear and transparent representation of gradient shaping within the code. The decision to combine FRIENDS with gradient shaping is justified by the latter's focus on linear regression models, providing a more cohesive and effective defense against adversarial attacks.

Acknowledging the study's limitations, we ground claims of proposing a comprehensive defense approach in a diversified experimental setup. The FRIENDS method has been applied to various datasets [4], considering different ratios of malicious data points, diverse functions, and model complexities. Addressing the non-generalizability of the analysis and providing transparency regarding the assumptions made contribute to a more thorough evaluation of the proposed defense approach. The FRIENDS method aims to achieve greater clarity, transparency, and effectiveness in countering adversarial attacks while addressing specific concerns raised in the discussion.

The provided code assumes the presence of a CSV file ('winequality-white.csv') containing input and output features, with the former normalized and split into training and testing sets. The model is initialized with parameters, and a combined optimizer, blending Adam and Stochastic Gradient Descent, is used for training. The code introduces noise in the form of Laplace and Gaussian distributions during gradient calculations. A poisoning attack is simulated by appending a malicious data point to the training set, and the resulting model is evaluated on a testing set for multiple trials. Trial details and Mean Squared Error (MSE) of the poisoned model are recorded in a CSV file ('poisoning_trials.csv').

# Data Evaluation
The initial variable declarations for noise_std, clip_threshold, epochs, and learning_rate were set to 5, 1, 100000, and 0.001 respectively.

The first variable we fine-tuned was the standard deviation of noise. The initial value, set to an integer 5, led to an attack success rate of 10 % and an average MSE  of 0.78. Doubling it (setting the value to an integer 10) led to an attack success rate of 16 % and an average MSE of 0.947. The increase showed that a greater number for noise led to a decrease in accuracy. Halving the initial value (setting the value to a float 2.5) led to a decrease in accuracy as well, with an attack success rate of 16 % and an average MSE of 0.914.

The second variable to adjust was the clipping threshold. The initial value’s results are given above. Doubling it (setting the value to an integer 2) led to an  attack success rate  of 6 %, showing a promising amount of decrease in the MSE as well, 0.566. Halving the initial value (setting the value to a float 0.5) led to a decrease in accuracy, with an attack success rate of 10 % and an average MSE of 0.851.

The third variable to tweak was the number of epochs. The initial value’s results are given above. Doubling it (setting the value to an integer 200000) led to an  attack success rate  of 20 %, showing an increase in the MSE, at 0.8996. Halving the initial value (setting the value to an integer 50000) led to a decrease in accuracy as well, with an attack success rate of 10 % and an average MSE of 0.78126.

The learning rate yielded similar results, but the value was multiplied and divided by 10, to test for a bigger margin of values. Dividing the initial learning rate by 10 (float value of 0.0001) showed a significant increase in MSE, 1.1253 with an attack success rate of  20 %. Multiplying the initial learning rate by 10 (float value of 0.1) showed an MSE of 0.887 and an attack success rate of 12 %.

Our conclusion was that noise_std should be set to 5, clip_threshold should be set to 2.0, the number of epochs should be at 100000, and the learning_rate should be set to 0.001. The complete trials and data are listed: https://docs.google.com/spreadsheets/d/1ygUmPWV5QD8Wu7padrqGg5HE3nJMieX_I5Ea5PGT8wE/edit?usp=sharing

# Conclusion and Future Work
Python has become a dominant programming language in AI due to  its versatility and rich libraries. Adversarial ML is now a critical  aspect of AI, addressing concerns about security and malicious attacks on models. Our open-source Python program specifically  focused on defending against Poisoning attacks works to combat some of these challenges. By combining Gradient Shaping and FRIENDS algorithms, our program maximizes defense against malicious data insertion during model training, ensuring robust  protection for linear regression based AI systems. 

The program allows for accessibility and collaboration because it is open-source.  Developers and researchers are welcome to use and enhance our  code, contributing to its continual  improvement. Our defense mechanism, built on Python 3.0, ensures an efficient and industry-standard implementation. 

Looking ahead, we are optimistic about the future potential of this project. We envision expanding its application to diverse machine learning programs and integrating it with advanced malicious data detection algorithms. The journey to enhance AI security remains ongoing, and we are dedicated  to advancing our program to fortify AI systems against emerging adversarial risks, protecting  their integrity across various  domains. As Python  continues to drive AI innovations, our project serves as a promising milestone in secure AI models.

# References
[1] S. Hong, V. Chandrasekaran, Y. Kaya, T. Dumitras, and N. Papernot, “On the effectiveness of mitigating data poisoning attacks with gradient shaping,” arXiv preprint arXiv:2002.11497, 2020.

[2] Lu, T. Y., Yang, Y., & Mirzasoleiman, B. (n.d.). Friendly noise against adversarial noise: A powerful defense ... - cs. https://web.cs.ucla.edu/~baharan/papers/liu22friendly.pdf 

[3] Xiong, P., Buffet, S., Iqbal, S., Lamontage, P., Mamun, M., & Molyneaux, H. (n.d.). Towards a robust and trustworthy 
machine learning system ... - arxiv.org. https://arxiv.org/pdf/2101.03042.pdf 

[4] Cortez,Paulo, Cerdeira,A., Almeida,F., Matos,T., and Reis,J.. (2009). Wine Quality. UCI Machine Learning Repository. https://doi.org/10.24432/C56S3T.

